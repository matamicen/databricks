{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3fe3c98-6d7c-49b3-8b85-faa3fa668eef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mount point is already mounted.\n"
     ]
    }
   ],
   "source": [
    "# Define the mount point you want to check\n",
    "mount_point_to_check = '/mnt/skynetdev'\n",
    "\n",
    "# Get the list of currently mounted file systems\n",
    "mounted_file_systems = dbutils.fs.mounts()\n",
    "\n",
    "# Check if the mount point already exists in the mounted file systems\n",
    "is_mounted = any(mount_point_to_check == mount_info.mountPoint for mount_info in mounted_file_systems)\n",
    "\n",
    "# Perform the mount only if it's not already mounted\n",
    "if not is_mounted:\n",
    "    dbutils.fs.mount(\n",
    "        source='wasbs://input@skynetdev.blob.core.windows.net',\n",
    "        mount_point='/mnt/skynetdev',\n",
    "        extra_configs={'fs.azure.account.key.skynetdev.blob.core.windows.net':'Fxcl6jPU7hOWgCToYUH4sA2RFUqJJ7rHHx9rA7euvHDxNgCeyfxHjRBxdRRIAwPktQlepHw0/zm7+AStZ/7g3A=='}\n",
    "    )\n",
    "else:\n",
    "    print(\"The mount point is already mounted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3994a96a-7b1a-4b90-974c-5c8798b2f312",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-----------+-----+-------+\n|CustomerID|   CustomerName|       City|State|Country|\n+----------+---------------+-----------+-----+-------+\n|         1|     Mark Perry|Los Angeles|   CA|    USA|\n|        13|Esetban3 Quito3|Los Angeles|   CA|    USA|\n+----------+---------------+-----------+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/mnt/skynetdev/sales_star_schema/customer.csv\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ed655e7-f5ba-446a-8fc0-9b719c6ddb80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mount point is already mounted.\n"
     ]
    }
   ],
   "source": [
    "# Define the mount point you want to check\n",
    "mount_point_to_check = '/mnt/skynet/output'\n",
    "\n",
    "# Get the list of currently mounted file systems\n",
    "mounted_file_systems = dbutils.fs.mounts()\n",
    "\n",
    "# Check if the mount point already exists in the mounted file systems\n",
    "is_mounted = any(mount_point_to_check == mount_info.mountPoint for mount_info in mounted_file_systems)\n",
    "\n",
    "if not is_mounted:\n",
    "    # Define the client secret as a Databricks secret (Optional: if you are using a secret)\n",
    "    client_secret = dbutils.secrets.get(scope=\"enter-your-key-vault-secret-scope-name-here\", key=\"enter-the-secret\")\n",
    "\n",
    "    # Define the configurations for the mount\n",
    "    configs = {\n",
    "        \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "        \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "        \"fs.azure.account.oauth2.client.id\": \"d32332e2-ee48-4254-95b9-eb9d7ead2b1c\",\n",
    "        \"fs.azure.account.oauth2.client.secret\": client_secret,  # Use the Databricks secret here\n",
    "        \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/26c2ce68-82a1-455e-981c-ef9bd4dffc2f/oauth2/token\"\n",
    "    }\n",
    "\n",
    "    # Mount the ADLS Gen2 container\n",
    "    dbutils.fs.mount(\n",
    "        source=\"abfss://output@skynetdev.dfs.core.windows.net\",\n",
    "        mount_point=\"/mnt/skynet/output\",\n",
    "        extra_configs=configs\n",
    "    )\n",
    "else:\n",
    "    print(\"The mount point is already mounted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5594b316-e8d6-4203-aee4-55c8aaf928fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from delta import *\n",
    "\n",
    "# # Initialize SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Save CSV to Delta Parquet\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# # Load the customer.csv file\n",
    "# input_path = \"dbfs:/mnt/skynetdev/sales_star_schema/customer.csv\"\n",
    "# df = spark.read.format(\"csv\").option(\"header\", \"true\").load(input_path)\n",
    "\n",
    "# # Show the loaded data (optional)\n",
    "# df.show()\n",
    "\n",
    "# # Save the DataFrame in Delta Parquet format\n",
    "# output_path = \"dbfs:/mnt/skynet/output/adb_customer\"\n",
    "# df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dcd73a8-2766-4920-aa5c-cbfced9ab8c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-----------+-----+-------+\n|CustomerID|   CustomerName|       City|State|Country|\n+----------+---------------+-----------+-----+-------+\n|         1|     Mark Perry|Los Angeles|   CA|    USA|\n|        13|Esetban3 Quito3|Los Angeles|   CA|    USA|\n+----------+---------------+-----------+-----+-------+\n\n+----------+---------------+------------+-----+---------+----------+----------+\n|CustomerID|   CustomerName|        City|State|  Country| StartDate|   EndDate|\n+----------+---------------+------------+-----+---------+----------+----------+\n|         3|Michael Johnson|     Chicago|   IL|      USA|2023-07-24|9999-12-31|\n|         4| Sarah Williams|     Houston|   TX|      USA|2023-07-24|9999-12-31|\n|         5|    David Brown|       Miami|   FL|      USA|2023-07-24|9999-12-31|\n|         6|    Emily Davis|     Seattle|   WA|      USA|2023-07-24|9999-12-31|\n|         7|  Robert Wilson|      Boston|   MA|      USA|2023-07-24|9999-12-31|\n|         8| Linda Thompson|      Dallas|   TX|      USA|2023-07-24|9999-12-31|\n|         9|    William Lee|     Atlanta|   GA|      USA|2023-07-24|9999-12-31|\n|        10| Susan Anderson|      Denver|   CO|      USA|2023-07-24|9999-12-31|\n|         1|     Mark Perry|Buenos Aires|   BA|Argentina|2023-07-24|2023-07-28|\n|        13|Esetban3 Quito3| Los Angeles|   CA|      USA|2023-07-28|9999-12-31|\n|         2|       Jane Doe| Los Angeles|   CA|      USA|2023-07-24|2023-07-24|\n+----------+---------------+------------+-----+---------+----------+----------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>CustomerID</th><th>CustomerName</th><th>City</th><th>State</th><th>Country</th><th>StartDate</th><th>EndDate</th></tr></thead><tbody><tr><td>3</td><td>Michael Johnson</td><td>Chicago</td><td>IL</td><td>USA</td><td>2023-07-24</td><td>9999-12-31</td></tr><tr><td>4</td><td>Sarah Williams</td><td>Houston</td><td>TX</td><td>USA</td><td>2023-07-24</td><td>9999-12-31</td></tr><tr><td>5</td><td>David Brown</td><td>Miami</td><td>FL</td><td>USA</td><td>2023-07-24</td><td>9999-12-31</td></tr><tr><td>6</td><td>Emily Davis</td><td>Seattle</td><td>WA</td><td>USA</td><td>2023-07-24</td><td>9999-12-31</td></tr><tr><td>7</td><td>Robert Wilson</td><td>Boston</td><td>MA</td><td>USA</td><td>2023-07-24</td><td>9999-12-31</td></tr><tr><td>8</td><td>Linda Thompson</td><td>Dallas</td><td>TX</td><td>USA</td><td>2023-07-24</td><td>9999-12-31</td></tr><tr><td>9</td><td>William Lee</td><td>Atlanta</td><td>GA</td><td>USA</td><td>2023-07-24</td><td>9999-12-31</td></tr><tr><td>10</td><td>Susan Anderson</td><td>Denver</td><td>CO</td><td>USA</td><td>2023-07-24</td><td>9999-12-31</td></tr><tr><td>1</td><td>Mark Perry</td><td>Buenos Aires</td><td>BA</td><td>Argentina</td><td>2023-07-24</td><td>2023-07-28</td></tr><tr><td>13</td><td>Esetban3 Quito3</td><td>Los Angeles</td><td>CA</td><td>USA</td><td>2023-07-28</td><td>9999-12-31</td></tr><tr><td>2</td><td>Jane Doe</td><td>Los Angeles</td><td>CA</td><td>USA</td><td>2023-07-24</td><td>2023-07-24</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "3",
         "Michael Johnson",
         "Chicago",
         "IL",
         "USA",
         "2023-07-24",
         "9999-12-31"
        ],
        [
         "4",
         "Sarah Williams",
         "Houston",
         "TX",
         "USA",
         "2023-07-24",
         "9999-12-31"
        ],
        [
         "5",
         "David Brown",
         "Miami",
         "FL",
         "USA",
         "2023-07-24",
         "9999-12-31"
        ],
        [
         "6",
         "Emily Davis",
         "Seattle",
         "WA",
         "USA",
         "2023-07-24",
         "9999-12-31"
        ],
        [
         "7",
         "Robert Wilson",
         "Boston",
         "MA",
         "USA",
         "2023-07-24",
         "9999-12-31"
        ],
        [
         "8",
         "Linda Thompson",
         "Dallas",
         "TX",
         "USA",
         "2023-07-24",
         "9999-12-31"
        ],
        [
         "9",
         "William Lee",
         "Atlanta",
         "GA",
         "USA",
         "2023-07-24",
         "9999-12-31"
        ],
        [
         "10",
         "Susan Anderson",
         "Denver",
         "CO",
         "USA",
         "2023-07-24",
         "9999-12-31"
        ],
        [
         "1",
         "Mark Perry",
         "Buenos Aires",
         "BA",
         "Argentina",
         "2023-07-24",
         "2023-07-28"
        ],
        [
         "13",
         "Esetban3 Quito3",
         "Los Angeles",
         "CA",
         "USA",
         "2023-07-28",
         "9999-12-31"
        ],
        [
         "2",
         "Jane Doe",
         "Los Angeles",
         "CA",
         "USA",
         "2023-07-24",
         "2023-07-24"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "CustomerID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CustomerName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "City",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "State",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "StartDate",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "EndDate",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from delta import *\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Save CSV to Delta Parquet with SCD Type 2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the customer.csv file\n",
    "input_path = \"dbfs:/mnt/skynetdev/sales_star_schema/customer.csv\"\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(input_path)\n",
    "\n",
    "# Show the loaded data (optional)\n",
    "df.show()\n",
    "\n",
    "# Define the output path for Delta Lake\n",
    "output_path = \"dbfs:/mnt/skynet/output/adb_customer_delta\"\n",
    "\n",
    "# Check if the Delta table already exists\n",
    "if DeltaTable.isDeltaTable(spark, output_path):\n",
    "    # If the Delta table exists, read the current data from the table\n",
    "    delta_table = DeltaTable.forPath(spark, output_path)\n",
    "    current_data = delta_table.toDF()\n",
    "    # print(\"current records:\")\n",
    "    current_data.show()\n",
    "\n",
    "    # Get the new records to be inserted\n",
    "    new_records = df.alias(\"new\").join(current_data.alias(\"current\"), \"CustomerID\", \"left_outer\") \\\n",
    "        .where(F.col(\"current.CustomerID\").isNull()) \\\n",
    "        .selectExpr(\"new.*\")\n",
    "\n",
    "    # Get the records to be updated\n",
    "    records_to_update = df.alias(\"new\").join(current_data.alias(\"current\"), \"CustomerID\") \\\n",
    "        .where(\n",
    "            (F.col(\"new.CustomerName\") != F.col(\"current.CustomerName\")) |\n",
    "            (F.col(\"new.City\") != F.col(\"current.City\")) |\n",
    "            (F.col(\"new.State\") != F.col(\"current.State\")) |\n",
    "            (F.col(\"new.Country\") != F.col(\"current.Country\"))\n",
    "        ) \\\n",
    "        .selectExpr(\"new.*\")\n",
    "\n",
    "\n",
    "    # Set the end date for the current version of the records to be updated\n",
    "    records_to_update = records_to_update.withColumn(\"EndDate\", F.current_date())\n",
    "\n",
    "    records_to_update = records_to_update.withColumn(\"CustomerID\", F.col(\"CustomerID\").cast(\"string\"))\n",
    "\n",
    "\n",
    "    # Update the current version of the records in the Delta table\n",
    "    delta_table.alias(\"current\").merge(\n",
    "        records_to_update.alias(\"updates\"),\n",
    "        \"current.CustomerID = updates.CustomerID\"\n",
    "    ).whenMatchedUpdate(set={\n",
    "        \"EndDate\": F.col(\"updates.EndDate\"),\n",
    "        \"CustomerName\": F.col(\"updates.CustomerName\")\n",
    "    }).execute()\n",
    "\n",
    "    # Insert the new records into the Delta table\n",
    "    delta_table.alias(\"current\").merge(\n",
    "        new_records.alias(\"new\"),\n",
    "        \"current.CustomerID = new.CustomerID\"\n",
    "    ).whenNotMatchedInsert(values={\n",
    "        \"CustomerID\": F.col(\"new.CustomerID\"),\n",
    "        \"CustomerName\": F.col(\"new.CustomerName\"),\n",
    "        \"City\": F.col(\"new.City\"),\n",
    "        \"State\": F.col(\"new.State\"),\n",
    "        \"Country\": F.col(\"new.Country\"),\n",
    "        \"StartDate\": F.current_date(),\n",
    "        \"EndDate\": F.lit(\"9999-12-31\")\n",
    "    }).execute()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # delta_table.toDF().show()\n",
    "    display(delta_table.toDF())\n",
    "\n",
    "\n",
    "else:\n",
    "    # If the Delta table does not exist, save the DataFrame as a new Delta table\n",
    "    # Adding the \"StartDate\" and \"EndDate\" columns to the schema\n",
    "    df = df.withColumn(\"StartDate\", F.current_date())\n",
    "    df = df.withColumn(\"EndDate\", F.lit(\"9999-12-31\"))\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "CustomerCSV2DeltaParquet",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
