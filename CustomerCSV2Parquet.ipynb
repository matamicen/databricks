{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3fe3c98-6d7c-49b3-8b85-faa3fa668eef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mount point is already mounted.\n"
     ]
    }
   ],
   "source": [
    "# Define the mount point you want to check\n",
    "mount_point_to_check = '/mnt/skynetdev'\n",
    "\n",
    "# Get the list of currently mounted file systems\n",
    "mounted_file_systems = dbutils.fs.mounts()\n",
    "\n",
    "# Check if the mount point already exists in the mounted file systems\n",
    "is_mounted = any(mount_point_to_check == mount_info.mountPoint for mount_info in mounted_file_systems)\n",
    "\n",
    "# Perform the mount only if it's not already mounted\n",
    "if not is_mounted:\n",
    "    dbutils.fs.mount(\n",
    "        source='wasbs://input@skynetdev.blob.core.windows.net',\n",
    "        mount_point='/mnt/skynetdev',\n",
    "        extra_configs={'fs.azure.account.key.skynetdev.blob.core.windows.net':'Fxcl6jPU7hOWgCToYUH4sA2RFUqJJ7rHHx9rA7euvHDxNgCeyfxHjRBxdRRIAwPktQlepHw0/zm7+AStZ/7g3A=='}\n",
    "    )\n",
    "else:\n",
    "    print(\"The mount point is already mounted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3994a96a-7b1a-4b90-974c-5c8798b2f312",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-----------+-----+-------+\n|CustomerID|   CustomerName|       City|State|Country|\n+----------+---------------+-----------+-----+-------+\n|         1|     Mark Perry|Los Angeles|   CA|    USA|\n|        13|Esetban3 Quito3|Los Angeles|   CA|    USA|\n+----------+---------------+-----------+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df_1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/mnt/skynetdev/sales_star_schema/customer.csv\")\n",
    "df_1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ed655e7-f5ba-446a-8fc0-9b719c6ddb80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mount point is already mounted.\n"
     ]
    }
   ],
   "source": [
    "# Define the mount point you want to check\n",
    "mount_point_to_check = '/mnt/skynet/output'\n",
    "\n",
    "# Get the list of currently mounted file systems\n",
    "mounted_file_systems = dbutils.fs.mounts()\n",
    "\n",
    "# Check if the mount point already exists in the mounted file systems\n",
    "is_mounted = any(mount_point_to_check == mount_info.mountPoint for mount_info in mounted_file_systems)\n",
    "\n",
    "if not is_mounted:\n",
    "    # Define the client secret as a Databricks secret (Optional: if you are using a secret)\n",
    "    client_secret = dbutils.secrets.get(scope=\"enter-your-key-vault-secret-scope-name-here\", key=\"enter-the-secret\")\n",
    "\n",
    "    # Define the configurations for the mount\n",
    "    configs = {\n",
    "        \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "        \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "        \"fs.azure.account.oauth2.client.id\": \"d32332e2-ee48-4254-95b9-eb9d7ead2b1c\",\n",
    "        \"fs.azure.account.oauth2.client.secret\": client_secret,  # Use the Databricks secret here\n",
    "        \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/26c2ce68-82a1-455e-981c-ef9bd4dffc2f/oauth2/token\"\n",
    "    }\n",
    "\n",
    "    # Mount the ADLS Gen2 container\n",
    "    dbutils.fs.mount(\n",
    "        source=\"abfss://output@skynetdev.dfs.core.windows.net\",\n",
    "        mount_point=\"/mnt/skynet/output\",\n",
    "        extra_configs=configs\n",
    "    )\n",
    "else:\n",
    "    print(\"The mount point is already mounted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13dbd171-c1d6-40f7-9a82-29d1d7144f1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-----------+-----+-------+\n|CustomerID|   CustomerName|       City|State|Country|\n+----------+---------------+-----------+-----+-------+\n|         1|     Mark Perry|Los Angeles|   CA|    USA|\n|        13|Esetban3 Quito3|Los Angeles|   CA|    USA|\n+----------+---------------+-----------+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Save CSV to Parquet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the customer.csv file\n",
    "input_path = \"dbfs:/mnt/skynetdev/sales_star_schema/customer.csv\"\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(input_path)\n",
    "\n",
    "# Show the loaded data (optional)\n",
    "df.show()\n",
    "\n",
    "# Save the DataFrame in Parquet format\n",
    "output_path = \"dbfs:/mnt/skynet/output/adb_customer\"\n",
    "# df.write.parquet(output_path, mode=\"overwrite\")\n",
    "df.write.parquet(output_path, mode=\"append\")\n",
    "\n",
    "\n",
    "# Stop the SparkSession (optional if you don't need it for further processing)\n",
    "# spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "CustomerCSV2Parquet",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
